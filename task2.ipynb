{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CheckThat Lab Task 2: Claims Extraction & Normalization (English)\n",
    "\n",
    "In this task, you will be given a noisy, unstructured social media post, and your goal is to simplify it into a concise form, and normalize them into a structured format. \n",
    "\n",
    "Therefore, we aim to bridge this gap by decomposing social media posts into simpler, more comprehensible forms, which are referred to as normalized claims.\n",
    "\n",
    "We will employ METEOR score for final evaluation.\n",
    "\n",
    "For more information, please visit [CHECKTHAT! LAB TASK 2](https://checkthat.gitlab.io/clef2025/task2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Claim Normalization\n",
    "\n",
    "Given a noisy, unstructured social media post, the task is to simplify it into a concise form.\n",
    "This is a text generation task in which systems have to generate the normlized claims for the goven social media posts.\n",
    "\n",
    "# Steps to run the code\n",
    "\n",
    "## 1. Start by processing the dataset to match the instruction finetuning format for the together.ai platform\n",
    "```bash\n",
    "python3 process_data.py\n",
    "```\n",
    "Run the above command on CLI and this will generate a JSONL file with reformatted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "import json\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "def read_prompt(path: Any) -> str:\n",
    "    \"\"\"\n",
    "    Read the system prompt from the file and return the prompt as a string.\n",
    "\n",
    "    Args:\n",
    "        path (str): path to the file containing the system prompt.\n",
    "    \"\"\"\n",
    "    file_path: str\n",
    "    if path is None:\n",
    "        file_path = \"./prompt.jsonl\"\n",
    "    else:\n",
    "        file_path = path\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            json_object = json.load(file)\n",
    "            PROMPT = json_object['prompt']\n",
    "            return PROMPT\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No file found for the prompt.\\nPlease re-launch the program using python3 process_data.py -p path_to_prompt_file\")\n",
    "        exit()\n",
    "        \n",
    "def read_data(path: Any) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read the data from the file and return the data as a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        path (str): path to the file containing the data.\n",
    "    \"\"\"\n",
    "    fp: str\n",
    "    if path is None:\n",
    "        fp = \"./data/train.csv\"\n",
    "    else:\n",
    "        fp = path\n",
    "    try:\n",
    "        data = pd.read_csv(fp)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"No file found for the data.\\nPlease re-launch the program using python3 process_data.py -d path_to_data_file\")\n",
    "        exit()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data: pd.DataFrame, prompt: str) -> Any:\n",
    "    \"\"\"\n",
    "    Process the data to re-format it to match instruction fine-tuning data style.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): data to be processed.\n",
    "        prompt (str): system prompt for the fine-tuning data.\n",
    "    \"\"\"\n",
    "    train_examples = []\n",
    "    output_file = \"./data/buffer_data.jsonl\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for index, item in data.iterrows():\n",
    "            json_data = {\n",
    "                \"messages\": [\n",
    "                                {\n",
    "                                    \"content\":prompt,\n",
    "                                    \"role\": \"system\"\n",
    "                                },\n",
    "                                {\n",
    "                                    \"content\": item[\"post\"],\n",
    "                                    \"role\": \"user\"\n",
    "                                },\n",
    "                                {\n",
    "                                    \"content\": item[\"normalized claim\"],\n",
    "                                    \"role\": \"assistant\"\n",
    "                                }\n",
    "                            ]\n",
    "            }\n",
    "            try:\n",
    "                # Validate JSON by encoding and decoding\n",
    "                json_line = json.dumps(json_data, ensure_ascii=False)\n",
    "                json.loads(json_line)  # This will raise an error if JSON is invalid\n",
    "                f.write(json_line + '\\n')\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error in line {index + 1}: {e}\")\n",
    "                \n",
    "    with open(output_file, 'r', encoding='utf-8') as in_file, open(\"./data/finetune_data.jsonl\", 'w', encoding='utf-8') as outfile:\n",
    "        for line_number, line in enumerate(in_file, 1):\n",
    "            try:\n",
    "                # Try to parse the JSON\n",
    "                json_object = json.loads(line.strip())\n",
    "\n",
    "                # Write the valid JSON line to the output file\n",
    "                json.dump(json_object, outfile)\n",
    "                outfile.write('\\n')\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error in line {line_number}: {e}\")\n",
    "    os.remove(output_file)\n",
    "    return outfile.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT: str\n",
    "file_path: str\n",
    "TRAIN_DATA: pd.DataFrame\n",
    "    \n",
    "PROMPT = read_prompt(None)\n",
    "TRAIN_DATA = read_data(None)\n",
    "    \n",
    "FINETUNE_DATA = process_data(TRAIN_DATA, PROMPT)\n",
    "print(f\"Fine-tuning data has been created and saved in the file: {FINETUNE_DATA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload the file to together.ai's fine-tuning queue.\n",
    "    \n",
    "First set your account's API key to an environment variable named TOGETHER_API_KEY:\n",
    "    \n",
    "```bash\n",
    "export TOGETHER_API_KEY=xxxxx\n",
    "```\n",
    "    \n",
    "Install together library \n",
    "    \n",
    "```bash\n",
    "pip install together --upgrade\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from together import Together\n",
    "from typing import Any\n",
    "import argparse\n",
    "\n",
    "def uploadfile(filepath:Any):\n",
    "    file_name: str\n",
    "    if filepath is None:\n",
    "        file_name = \"./data/finetune_data.jsonl\"\n",
    "    else:\n",
    "        file_name = filepath\n",
    "    try:\n",
    "        client = Together(api_key=os.getenv(\"TOGETHER_API_KEY\"))\n",
    "        file_resp = client.files.upload(file=file_name, check=True)\n",
    "        return file_resp.model_dump()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        if e is FileNotFoundError:\n",
    "            print(f\"No file found for the data.\\nPlease re-launch the program using python3 process_data.py -d path_to_data_file\")\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_response: Any\n",
    "upload_response = uploadfile(None)        \n",
    "print(f\"File upload response:\\n{upload_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a fine-tuning job on together.ai\n",
    "Run the below command to create a fine-tuning job on together.ai\n",
    "\n",
    "### Python\n",
    "```bash\n",
    "python3 together_finetune.py -m model_name -f file-id\n",
    "```\n",
    "### CLI\n",
    "```bash\n",
    "together fine-tuning create \\\n",
    "  --training-file \"file-629e58b4-ff73-438c-b2cc-f69542b27980\" \\\n",
    "  --model \"meta-llama/Meta-Llama-3.1-8B-Instruct-Reference\" \\\n",
    "  --lora\n",
    "```\n",
    "The response object will have all the details of your job, including its ID and a `status` key that starts out as \"pending\":\n",
    "```bash\n",
    "{\n",
    "  id='ft-66592697-0a37-44d1-b6ea-9908d1c81fbd', \n",
    "  training_file='file-63b9f097-e582-4d2e-941e-4b541aa7e328', \n",
    "  validation_file='', \n",
    "  model='meta-llama/Meta-Llama-3.1-8B-Instruct-Reference', \n",
    "  output_name='zainhas/Meta-Llama-3.1-8B-Instruct-Reference-30b975fd', \n",
    "... \n",
    "  status=<FinetuneJobStatus.STATUS_PENDING: 'pending'>\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Monitoring the fine-tuning status\n",
    "Go to your Dashboard on togther.ai and look under jobs to monitor the fine-tuning progress. Alternatively, you can also use the below command to get the status of the job.\n",
    "```bash\n",
    "together fine-tuning retrieve \"ft-66592697-0a37-44d1-b6ea-9908d1c81fbd\"\n",
    "```\n",
    "Your fine-tuning job will go through several phases, including `Pending` , `Queued` , `Running` , `Uploading` , and `Completed` .\n",
    "## 5. Download Checkpoints\n",
    "Once the fine-tuning jo is completed, download the Adapter checkpoints to run locally with your base model.\n",
    "\n",
    "## 6. Downloading model from hugging face\n",
    "Download the base version of your chosen model from hugging face\n",
    "\n",
    "Authenticate yourself first by logging into your Hugging Face account\n",
    "```bash\n",
    "huggingface-cli login\n",
    "```\n",
    "Run the below command to download the model\n",
    "```bash\n",
    "huggingface-cli download meta-llama/Meta-Llama-3.1-8B-Instruct --include \"original/*\" --local-dir meta-llama/Meta-Llama-3.1-8B-Instruct\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run local Inference \n",
    "To evaluate the performance on the validation set, run inference locally using the below command\n",
    "```bash\n",
    "python3 evaluate.py\n",
    "```\n",
    "Make sure you update the paths to the model and adapters in your evaluate.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from together import Together\n",
    "from typing import Any\n",
    "import nltk\n",
    "nltk.download('wordnet', quiet=True)\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "def get_claim(model: Any, tokenizer: Any, user_prompt: str) -> str:\n",
    "    sys_promt = \"\"\"You are a helpful AI assistant that can generate a summary of claims made in a given text in the style of a news headline.\n",
    "    Based on the input text, extract a claim that is being made or implied and return it as a json object.\"\"\"\n",
    "    inputs = tokenizer(f\"{sys_promt}\\ninput_text:{user_prompt}\", return_tensors=\"pt\").to('cuda')\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=30,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True, \n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    generated_claim = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    torch.cuda.empty_cache()\n",
    "    return generated_claim\n",
    "\n",
    "def evaluate_model(model: Any, tokenizer: Any, input_data: Any):\n",
    "    scores = []\n",
    "    responses = []\n",
    "    for index, item in tqdm(input_data.iterrows(), total=len(input_data)):\n",
    "        response = get_claim(model, tokenizer, item['post'])\n",
    "        #print(f\"Response: {response}\")\n",
    "        token_res = word_tokenize(response)\n",
    "        token_label = word_tokenize(item['normalized claim'])\n",
    "        scores.append(meteor_score([token_res], token_label))\n",
    "        responses.append(response)\n",
    "    with open(\"generated_claims_dev.jsonl\", \"w\") as f:\n",
    "        json.dump(responses,f, ensure_ascii=False)\n",
    "        \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path: str\n",
    "DEV_DATA: pd.DataFrame\n",
    "    \n",
    "file_path = \"./data/dev.csv\"\n",
    "DEV_DATA = pd.read_csv(file_path)\n",
    "    \n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "base_model_path = \"./meta-llama/Meta-Llama-3.1-8B-Instruct\"  # Local directory of the base model\n",
    "lora_adapter_path = \"./lora-adapters\"  # Path to your LoRA adapters\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path, local_files_only=True)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    local_files_only=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\" \n",
    ")\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model = PeftModel.from_pretrained(model, lora_adapter_path, is_trainable=False)\n",
    "model = model.to(device)\n",
    "    \n",
    "METEROR_SCORE = evaluate_model(model, tokenizer, DEV_DATA)\n",
    "    \n",
    "print(f\"Average METEOR Score: {METEROR_SCORE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n",
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n",
    "client = Together(api_key = TOGETHER_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline flow of the task2\n",
    "\n",
    "The pipeline flow for the task 2 is as follows:\n",
    "1. Data Preprocessing: Clean and preprocess the data by reformatting to the finetuning format for the model of your choice.\n",
    "2. Fine-tuning: Upload the dataset file and create a new fine-tuning job \n",
    "3. Evaluation: Use the model to extract claims from the social media posts and calculate the average METEOR score on the development set.\n",
    "4. Training: Train the fine-tuned model using the training loop if the performance is not satisfactory.\n",
    "5. Inference: Use the model to extract claims from the social media posts from the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n",
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n",
    "client = Together(api_key = TOGETHER_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LoRA on Together.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from together import Together\n",
    "\n",
    "client = Together(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
    "\n",
    "response = client.fine_tuning.create(\n",
    "  training_file = 'file-5e32a8e6-72b3-485d-ab76-71a73d9e1f5b',\n",
    "  model = 'meta-llama/Meta-Llama-3-8B',\n",
    "  lora = True,\n",
    "  n_epochs = 3,\n",
    "  n_checkpoints = 1,\n",
    "  batch_size = \"max\",\n",
    "  learning_rate = 1e-5,\n",
    "  suffix = 'my-demo-finetune',\n",
    "  wandb_api_key = '1a2b3c4d5e.......', # weights and biases API key to directly monitor the training process\n",
    "  lora_r=16,\n",
    "  lora_dropout=0.5,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRA with huggingface libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preparing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Prepare the model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying LoRA to the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Load and prepare your dataset\n",
    "dataset = load_dataset(\"your_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFT Trainer Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SFT Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY)\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a one-sentence bedtime story about a unicorn.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
