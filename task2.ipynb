{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CheckThat Lab Task 2: Claims Extraction & Normalization (English)\n",
    "\n",
    "In this task, you will be given a noisy, unstructured social media post, and your goal is to simplify it into a concise form, and normalize them into a structured format. \n",
    "\n",
    "Therefore, we aim to bridge this gap by decomposing social media posts into simpler, more comprehensible forms, which are referred to as normalized claims.\n",
    "\n",
    "We will employ METEOR score for final evaluation.\n",
    "\n",
    "For more information, please visit [CHECKTHAT! LAB TASK 2](https://checkthat.gitlab.io/clef2025/task2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing together.ai \n",
    "\n",
    "To interact with the `together.ai` platform, you will need to install the necessary libraries. \n",
    "\n",
    "We will use pytorch to fine-tune the model.\n",
    "\n",
    "You can install them using pip.\n",
    "\n",
    "Run the below cell only once when you setup the environment first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install together --upgrade\n",
    "#!pip install torch \n",
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the claims\n",
    "\n",
    "Your task is to write a Python function `extract_claims` that takes a noisy, unstructured social media post as input and returns a dictionary containing the extracted claims. The function should utilize the `together.ai` platform to simplify the post using any model of your choice preferably Meta Llama 3.3 and extract the relevant information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need copy your API_KEY from together.ai and set the environment variable using \n",
    "\n",
    "$env:TOGETHER_API_KEY = \"YOUR_API_KEY\"\n",
    "\n",
    "Instead you can also hard code it into the Together() function using Together(api_key=\"YOUR_API_KEY\").\n",
    "\n",
    "But hard coding a personal secret API key is not advisable and do so at your own risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "Load the data from the dataset file and print to inspect the format\n",
    "\n",
    "Format data to appropriate format for fine-tuning purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = \"./data/train/train-eng.csv\"\n",
    "DEV_DATA = \"./data/dev/dev-eng.csv\"\n",
    "TEST_DATA = \"./data/test/test-eng.csv\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_DATA)\n",
    "dev_df = pd.read_csv(DEV_DATA)\n",
    "test_df = pd.read_csv(TEST_DATA)\n",
    "\n",
    "train_data = train_df['post']\n",
    "train_labels = train_df['normalized claim']\n",
    "\n",
    "dev_data = dev_df['post']\n",
    "dev_labels = dev_df['normalized claim']\n",
    "\n",
    "test_data = test_df['post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['The Karnofsky Jewish family, who immigrated to the United States from Lithuania, employed a 7-year-old boy and adopted (so to speak) him into their home.  He was originally given homework to get food because he was a starving kid.  He remained under the Jewish families employ, until he was 12  Karnofsky gave him money to buy his first instrument, which was a common instrument in Jewish families.  They really admired his musical talent.Later, when he became a professional â€¦ See More The Karnofsky Jewish family, who immigrated to the United States from Lithuania, employed a 7-year-old boy and adopted (so to speak) him into their home.  He was originally given homework to get food because he was a starving kid.  He remained under the Jewish families employ, until he was 12  Karnofsky gave him money to buy his first instrument, which was a common instrument in Jewish families.  They really admired his musical talent.Later, when he became a professional â€¦ See More The Karnofsky Jewish family, who immigrated to the United States from Lithuania, employed a 7-year-old boy and adopted (so to speak) him into their home.  He was originally given homework to get food because he was a starving kid.  He remained under the Jewish families employ, until he was 12  Karnofsky gave him money to buy his first instrument, which was a common instrument in Jewish families.  They really admired his musical talent.Later, when he became a professional â€¦ See More None',\n",
       "       \"Trophy hunting is horrific. ðŸ˜¥ðŸ˜¥ðŸ˜¥ Trophy hunting is horrific. ðŸ˜¥ðŸ˜¥ðŸ˜¥ Trophy hunting is horrific. ðŸ˜¥ðŸ˜¥ðŸ˜¥ mill\\r\\nCo\\r\\nThis little guy was rescued after his\\r\\nmother was murdered by a hunter.\\r\\nHis eyes say it all, don't you think?\\r\\nIf you want to ban trophy hunting\\r\\nforever, spread this everywhere.\\r\\nImage Credit: FB. Changes: cropped, resized, text added. https://bit.ly/3b2vwas\",\n",
       "       'Magarmacch // Heavy Rain //Hyderabad //Crocodile // Alert Magarmacch // Heavy Rain //Hyderabad //Crocodile // Alert Magarmacch // Heavy Rain //Hyderabad //Crocodile // Alert None',\n",
       "       ...,\n",
       "       '24 Yearâ€™s Old Indian Muslim Girl Qualifies IAS in Maharashtra Mumbai',\n",
       "       'This is the â€˜kindly old manâ€™ who was â€˜pushedâ€™ in Buffalo,\" the June 9 post said, alongside a photo that shows Gugino with a U.S. Park Police officer. \"Mr. Gugino had been previously arrested 300 times. 82 times for incitement. Riots is what he does for a living.\"',\n",
       "       'The Supreme Court has observed that those who feed stray #dogs could be held liable if the canines attack people. The apex court also said that such people should be made responsible for #vaccination of stray dogs.'],\n",
       "      shape=(1171,), dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df['post'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "instruction = \"\"\"You are a helpful AI assistant that extracts a claim made in the input text provided to you. \n",
    "Take a look at the below example and follow the similar style and format for the response.\n",
    "Original post: â–ªï¸Fake snow on texas..\n",
    "Haarp used chemtrail snow.\n",
    "#OperationDarkWinter â„ï¸\n",
    "#WeatherModification \n",
    "â–ªï¸Fake snow on texas..\n",
    "Haarp used chemtrail snow.\n",
    "#OperationDarkWinter â„ï¸\n",
    "#WeatherModification \n",
    "â–ªï¸Fake snow on texas..\n",
    "Haarp used chemtrail snow.\n",
    "#OperationDarkWinter â„ï¸\n",
    "#WeatherModification None\n",
    "Normalized claim: Video shows snow in Texas is fake\"\"\"\n",
    "                \n",
    "formatted_data = []\n",
    "\n",
    "# Iterate through the DataFrame and create the desired format\n",
    "for index, row in train_df.iterrows():\n",
    "    entry = {\n",
    "        'instruction': instruction,\n",
    "        'input': row['post'],\n",
    "        'output': row['normalized claim']\n",
    "    }\n",
    "    formatted_data.append(entry)\n",
    "\n",
    "# Save the formatted data to a JSON file\n",
    "with open('instruction_data_0.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(formatted_data, json_file, indent=4, ensure_ascii=False)\n",
    "    \n",
    "from typing import List, Dict\n",
    "\n",
    "def create_training_jsonl(input_data: List[Dict], output_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Create a properly formatted JSONL file for Together AI fine-tuning\n",
    "    \"\"\"\n",
    "    train_examples = []\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for item in input_data:\n",
    "            json_data = {\n",
    "                \"messages\": [\n",
    "                                {\n",
    "                                    \"content\":item.get(\"instruction\",\"\"),\n",
    "                                    \"role\": \"system\"\n",
    "                                },\n",
    "                                {\n",
    "                                    \"content\": str(item.get(\"input\", \"\")),\n",
    "                                    \"role\": \"user\"\n",
    "                                },\n",
    "                                {\n",
    "                                    \"content\": str(item.get(\"output\", \"\")),\n",
    "                                    \"role\": \"assistant\"\n",
    "                                }\n",
    "                            ]\n",
    "            }\n",
    "            train_examples.append(json_data)\n",
    "            json_line = json.dumps(json_data, ensure_ascii=False)\n",
    "            f.write(json_line + '\\n')\n",
    "        #json.dump(train_examples, f, ensure_ascii=False)\n",
    "\n",
    "with open('instruction_data_0.json', 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "create_training_jsonl(data, 'instruction_data_0.jsonl')\n",
    "\n",
    "import json\n",
    "\n",
    "def validate_and_fix_jsonl(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for line_number, line in enumerate(infile, 1):\n",
    "            try:\n",
    "                # Try to parse the JSON\n",
    "                json_object = json.loads(line.strip())\n",
    "\n",
    "                # Write the valid JSON line to the output file\n",
    "                json.dump(json_object, outfile)\n",
    "                outfile.write('\\n')\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error in line {line_number}: {e}\")\n",
    "\n",
    "# Use the function\n",
    "validate_and_fix_jsonl('instruction_data_0.jsonl', 'fixed_instruction_data_0.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def create_training_jsonl(input_data: List[Dict], output_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Create a properly formatted JSONL file for Together AI fine-tuning\n",
    "    \"\"\"\n",
    "    train_examples = []\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for item in input_data:\n",
    "            json_data = {\n",
    "                \"messages\": [\n",
    "                                {\n",
    "                                    \"content\":item.get(\"instruction\",\"\"),\n",
    "                                    \"role\": \"system\"\n",
    "                                },\n",
    "                                {\n",
    "                                    \"content\": str(item.get(\"input\", \"\")),\n",
    "                                    \"role\": \"user\"\n",
    "                                },\n",
    "                                {\n",
    "                                    \"content\": str(item.get(\"output\", \"\")),\n",
    "                                    \"role\": \"assistant\"\n",
    "                                }\n",
    "                            ]\n",
    "            }\n",
    "            train_examples.append(json_data)\n",
    "            json_line = json.dumps(json_data, ensure_ascii=False)\n",
    "            f.write(json_line + '\\n')\n",
    "        #json.dump(train_examples, f, ensure_ascii=False)\n",
    "\n",
    "with open('instruction_data_0.json', 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "create_training_jsonl(data, 'instruction_data_0.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def validate_and_fix_jsonl(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for line_number, line in enumerate(infile, 1):\n",
    "            try:\n",
    "                # Try to parse the JSON\n",
    "                json_object = json.loads(line.strip())\n",
    "\n",
    "                # Write the valid JSON line to the output file\n",
    "                json.dump(json_object, outfile)\n",
    "                outfile.write('\\n')\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error in line {line_number}: {e}\")\n",
    "\n",
    "# Use the function\n",
    "validate_and_fix_jsonl('./data/finetune_data.jsonl', 'fixed_instruction_data_0.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n",
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n",
    "client = Together(api_key = TOGETHER_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload and check the files for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading file finetune_data.jsonl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11.3M/11.3M [00:05<00:00, 1.98MB/s]\n"
     ]
    }
   ],
   "source": [
    "file_resp = client.files.upload(file=\"./data/finetune_data.jsonl\", check=True)\n",
    "#print(file_resp.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available models\n",
    "models = client.models.list()\n",
    "for model in models:\n",
    "    #if hasattr(model., 'finetune'):\n",
    "    print(f\"Model: {model.display_name}, {model.pricing}\")\n",
    "#print(f\"Models available for fine-tuning: {models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "message='Starting from together>=1.3.0, the default batch size is set to the maximum allowed value for each model.'\n",
      "message='Starting from together>=1.3.0, the default batch size is set to the maximum allowed value for each model.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft-84a7748b\n"
     ]
    }
   ],
   "source": [
    "ft_resp = client.fine_tuning.create(\n",
    "    training_file = \"file-4a0981a6-2313-4453-84aa-0a1288235bfc\",\n",
    "    model = 'meta-llama/Llama-3.3-70B-Instruct-Reference',\n",
    "    train_on_inputs= \"auto\",\n",
    "    n_epochs = 3,\n",
    "    wandb_api_key = os.getenv('WANDB_API_KEY'),\n",
    "    lora = True,\n",
    "    learning_rate = 1e-5,\n",
    "    suffix = 'ZeroShot-FineTuned',\n",
    ")\n",
    "\n",
    "print(ft_resp.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nikhil_Kadapala/Llama-3.3-70B-Instruct-Reference-ZeroShot-FineTuned-bf15554a'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The output model name\n",
    "ft_resp.output_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'error', 'message': 'Error code: 404 - {\"message\": \"Invalid Cannot cancel job while in state cancelled param: status\", \"type_\": \"invalid_request_error\", \"param\": \"status\", \"code\": \"\"}'}\n"
     ]
    }
   ],
   "source": [
    "from together import Together\n",
    "\n",
    "client = Together(api_key=os.environ[\"TOGETHER_API_KEY\"])\n",
    "\n",
    "def cancel_finetune_job(job_id: str) -> dict:\n",
    "    try:\n",
    "        response = client.fine_tuning.cancel(id=job_id)\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"job_id\": job_id,\n",
    "            \"final_state\": response.get(\"status\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"message\": str(e)\n",
    "        }\n",
    "\n",
    "stats = cancel_finetune_job(\"ft-84a7748b\")\n",
    "\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ft-84a7748b is in terminal state FinetuneJobStatus.STATUS_CANCELLED\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import InvalidStateError\n",
    "\n",
    "job_id = \"ft-84a7748b\"\n",
    "job_status = client.fine_tuning.retrieve(job_id).status\n",
    "if job_status in [\"QUEUED\", \"RUNNING\"]:\n",
    "    print(f\"Job {job_id} is currently {job_status}\")\n",
    "else:\n",
    "    print(f\"Job {job_id} is in terminal state {job_status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_claims(model_name, input_data):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": input_data}],\n",
    "        max_tokens=None,\n",
    "        temperature=0.3,\n",
    "        top_p=0.7,\n",
    "        top_k=50,\n",
    "        repetition_penalty=1,\n",
    "        stop=[\"<|eot_id|>\"],\n",
    "        stream=False\n",
    "    )\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='914872eed81742cf' object=<ObjectType.ChatCompletion: 'chat.completion'> created=1739991093 model='Nikhil_Kadapala/Llama-3.2-1B-Instruct-ZeroShot-FineTuned-9836a5f1' choices=[ChatCompletionChoicesData(index=0, logprobs=None, seed=13488802899849945000, finish_reason=<FinishReason.EOS: 'eos'>, message=ChatCompletionMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content=\" \\nDonald Trump's presidency was marked by significant events, policies, and controversies. While opinions about his presidency vary widely, it is undeniable that he was a polarizing figure who left a lasting impact on the United States.\\n\\nSome of the key events and policies of his presidency include:\\n\\n1. The 2016 presidential election: Trump's victory marked a significant shift in American politics, with his campaign focusing on issues such as immigration, trade, and national security.\\n2. The travel ban: Trump issued a travel ban targeting predominantly Muslim countries, which sparked widespread protests and criticism from human rights groups.\\n3. The Iran nuclear deal: Trump withdrew the United States from the Joint Comprehensive Plan of Action (JCPOA), a landmark agreement aimed at limiting Iran's nuclear program.\\n4. The border wall: Trump proposed a massive border wall along the US-Mexico border, which was met with fierce opposition from Democrats and many Republicans.\\n5. The Supreme Court appointments: Trump appointed two Supreme Court justices, Neil Gorsuch and Brett Kavanaugh, who were confirmed by the Senate despite intense opposition from Democrats.\\n\\nDespite these significant events and policies, Trump's presidency was also marked by controversy and criticism. Some of the key criticisms include:\\n\\n1. The Russia investigation: Trump was accused of colluding with Russia during the 2016 presidential election, which led to a special counsel investigation and multiple convictions.\\n2. The Mueller investigation: Trump was impeached by the House of Representatives in 2019, but acquitted by the Senate in 2020.\\n3. The COVID-19 pandemic: Trump's response to the pandemic was widely criticized, with many accusing him of downplaying its severity and failing to take decisive action.\\n4. The economy: Trump's presidency was marked by a significant decline in economic growth, with the US GDP shrinking by 2.9% in 2020.\\n5. The social media platform controversy: Trump's use of Twitter to criticize opponents and promote his policies was widely criticized, with many accusing him of spreading misinformation and inciting violence.\\n\\nIn conclusion, while opinions about Trump's presidency are highly divided, it is undeniable that he was a polarizing figure who left a lasting impact on the United States. His presidency was marked by significant events and policies, as well as controversy and criticism.\", tool_calls=[]))] prompt=[] usage=UsageData(prompt_tokens=50, completion_tokens=462, total_tokens=512, cache_hit_rate=0)\n"
     ]
    }
   ],
   "source": [
    "model = \"Nikhil_Kadapala/Llama-3.2-1B-Instruct-ZeroShot-FineTuned-9836a5f1\"\n",
    "prompt = \"Donald Trump is the greatest president of the modern era. There is no doubt about it.\"\n",
    "response = get_claims(model, prompt)\n",
    "print(get_claims(\"Nikhil_Kadapala/Llama-3.2-1B-Instruct-ZeroShot-FineTuned-9836a5f1\", \"Donald Trump is the greatest president of the modern era. There is no doubt about it.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION METRICS\n",
    "\n",
    "NLTK'S METEOR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import numpy as np\n",
    "def meteor_loss(prediction, ground_truth):\n",
    "    return 1 - meteor_score([prediction], ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def evaluate_model(model_name, input_data, labels):\n",
    "    predictions = []\n",
    "    for data in tqdm(input_data):\n",
    "        response = get_claims(model_name, data)\n",
    "        predictions.append(response.choices[0].message.content)\n",
    "    score = meteor_score([predictions], labels)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"Nikhil_Kadapala/Llama-3.2-1B-Instruct-ZeroShot-FineTuned-9836a5f1\"\n",
    "avg_score = evaluate_model(model, dev_data[0:10], dev_labels[0:10])\n",
    "print(f\"Average METEOR score on development set: {avg_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "The training loop should follow these steps:\n",
    "\n",
    "1. Load the dataset of social media posts and their corresponding claims.\n",
    "\n",
    "2. Extract the claims from the input using your model of choice.\n",
    "\n",
    "3. Calculate the loss using the chosen metric (meteor).\n",
    "\n",
    "4. Update the model parameters using the optimizer.\n",
    "\n",
    "The training loop should continue for a specified number of epochs or until a certain stopping criterion is met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, optimizer, train_data, train_labels, criterion, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for i in tqdm(range(len(train_data))):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_data[i])\n",
    "            loss = criterion(output.choices[0].delta.content, train_labels[i])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_data)\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss = {train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"meta-llama/Llama-3.3-70B-Instruct-Turbo\"\n",
    "fine_tuned_1 =get_claims(model)\n",
    "epochs = 3\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(fine_tuned_1.parameters(), lr=lr)\n",
    "loss_fn = meteor_loss()\n",
    "train_model(fine_tuned_1, optimizer, train_data, train_labels, loss_fn, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "dev_loss = 0\n",
    "for i in range(len(dev_data)):\n",
    "    output = model(dev_data[i])\n",
    "    loss = meteor_loss(output.choices[0].delta.content, dev_labels[i])\n",
    "    dev_loss += loss.item()\n",
    "    dev_loss /= len(dev_data)\n",
    "    print(f\"Epoch {i + 1}: Dev Loss = {dev_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline flow of the task2\n",
    "\n",
    "The pipeline flow for the task 2 is as follows:\n",
    "1. Data Preprocessing: Clean and preprocess the data by reformatting to the finetuning format for the model of your choice.\n",
    "2. Fine-tuning: Upload the dataset file and create a new fine-tuning job \n",
    "3. Evaluation: Use the model to extract claims from the social media posts and calculate the average METEOR score on the development set.\n",
    "4. Training: Train the fine-tuned model using the training loop if the performance is not satisfactory.\n",
    "5. Inference: Use the model to extract claims from the social media posts from the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
